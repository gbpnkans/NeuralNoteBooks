{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An implementation of sequence to sequence learning for performing addition\\nInput: \"535+61\"\\nOutput: \"596\"\\nPadding is handled by using a repeated sentinel character (space)\\nInput may optionally be inverted, shown to increase performance in many tasks in:\\n\"Learning to Execute\"\\nhttp://arxiv.org/abs/1410.4615\\nand\\n\"Sequence to Sequence Learning with Neural Networks\"\\nhttp://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\\nTheoretically it introduces shorter term dependencies between source and target.\\nTwo digits inverted:\\n+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\\nThree digits inverted:\\n+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\\nFour digits inverted:\\n+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\\nFive digits inverted:\\n+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''An implementation of sequence to sequence learning for performing addition\n",
    "Input: \"535+61\"\n",
    "Output: \"596\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "Two digits inverted:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "Three digits inverted:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "Four digits inverted:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "Five digits inverted:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 9\n",
    "INVERT = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n"
     ]
    }
   ],
   "source": [
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if INVERT:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Training Data:\n",
      "(45000, 19, 12)\n",
      "(45000, 10, 12)\n",
      "Validation Data:\n",
      "(5000, 19, 12)\n",
      "(5000, 10, 12)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Using already trained model\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = None\n",
    "if os.path.isfile('./additionrnn_model.h5') and os.path.isfile('./additionrnn_weightsandbias.h5'):\n",
    "    model = load_model('additionrnn_model.h5')\n",
    "    model.load_weights('additionrnn_weightsandbias.h5')\n",
    "if model is None:\n",
    "    \n",
    "    model = Sequential()\n",
    "    # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "    # Note: In a situation where your input sequences have a variable length,\n",
    "    # use input_shape=(None, num_feature).\n",
    "    model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "    # As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "    # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "    # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "    model.add(layers.RepeatVector(DIGITS + 1))\n",
    "    # The decoder RNN could be multiple layers stacked or a single layer.\n",
    "    for _ in range(LAYERS):\n",
    "        # By setting return_sequences to True, return not only the last output but\n",
    "        # all the outputs so far in the form of (num_samples, timesteps,\n",
    "        # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "        # the first dimension to be the timesteps.\n",
    "        model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "    # Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "    # of the output sequence, decide which character should be chosen.\n",
    "    model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "    model.add(layers.Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "else:\n",
    "    print('Using already trained model')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 299s 7ms/step - loss: 0.0842 - acc: 0.9846 - val_loss: 0.0594 - val_acc: 0.9929\n",
      "Q 506310395+318872919 T 825183314  \u001b[92m☑\u001b[0m 825183314 \n",
      "Q 998480052+496663221 T 1495143273 \u001b[92m☑\u001b[0m 1495143273\n",
      "Q 575454437+885902923 T 1461357360 \u001b[92m☑\u001b[0m 1461357360\n",
      "Q 687443966+565039374 T 1252483340 \u001b[92m☑\u001b[0m 1252483340\n",
      "Q 7699116+55032555    T 62731671   \u001b[92m☑\u001b[0m 62731671  \n",
      "Q 923816476+283041963 T 1206858439 \u001b[92m☑\u001b[0m 1206858439\n",
      "Q 495015821+271467610 T 766483431  \u001b[92m☑\u001b[0m 766483431 \n",
      "Q 972055360+157656312 T 1129711672 \u001b[92m☑\u001b[0m 1129711672\n",
      "Q 703128496+868456942 T 1571585438 \u001b[92m☑\u001b[0m 1571585438\n",
      "Q 923941422+686184127 T 1610125549 \u001b[92m☑\u001b[0m 1610125549\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 298s 7ms/step - loss: 0.1467 - acc: 0.9660 - val_loss: 0.0555 - val_acc: 0.9932\n",
      "Q 377633766+72457232  T 450090998  \u001b[92m☑\u001b[0m 450090998 \n",
      "Q 380913312+386184458 T 767097770  \u001b[92m☑\u001b[0m 767097770 \n",
      "Q 451561547+388473052 T 840034599  \u001b[92m☑\u001b[0m 840034599 \n",
      "Q 541221729+392220577 T 933442306  \u001b[92m☑\u001b[0m 933442306 \n",
      "Q 125676744+222393235 T 348069979  \u001b[92m☑\u001b[0m 348069979 \n",
      "Q 590408832+69317076  T 659725908  \u001b[92m☑\u001b[0m 659725908 \n",
      "Q 188330321+118465171 T 306795492  \u001b[92m☑\u001b[0m 306795492 \n",
      "Q 232141252+876240994 T 1108382246 \u001b[92m☑\u001b[0m 1108382246\n",
      "Q 156571760+238191099 T 394762859  \u001b[91m☒\u001b[0m 394762769 \n",
      "Q 814415955+104401919 T 918817874  \u001b[91m☒\u001b[0m 918817974 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 306s 7ms/step - loss: 0.0449 - acc: 0.9959 - val_loss: 0.0384 - val_acc: 0.9971\n",
      "Q 502699301+568744292 T 1071443593 \u001b[92m☑\u001b[0m 1071443593\n",
      "Q 317091851+878048165 T 1195140016 \u001b[92m☑\u001b[0m 1195140016\n",
      "Q 391427335+140367652 T 531794987  \u001b[92m☑\u001b[0m 531794987 \n",
      "Q 611095408+409496505 T 1020591913 \u001b[92m☑\u001b[0m 1020591913\n",
      "Q 264407027+931595707 T 1196002734 \u001b[92m☑\u001b[0m 1196002734\n",
      "Q 875056111+48638762  T 923694873  \u001b[92m☑\u001b[0m 923694873 \n",
      "Q 951907834+793114635 T 1745022469 \u001b[92m☑\u001b[0m 1745022469\n",
      "Q 514687322+224316214 T 739003536  \u001b[92m☑\u001b[0m 739003536 \n",
      "Q 43347312+416058784  T 459406096  \u001b[92m☑\u001b[0m 459406096 \n",
      "Q 203424333+672666510 T 876090843  \u001b[92m☑\u001b[0m 876090843 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 298s 7ms/step - loss: 0.0660 - acc: 0.9882 - val_loss: 0.0434 - val_acc: 0.9945\n",
      "Q 932969449+823651029 T 1756620478 \u001b[92m☑\u001b[0m 1756620478\n",
      "Q 996099501+381149392 T 1377248893 \u001b[92m☑\u001b[0m 1377248893\n",
      "Q 53882573+422780448  T 476663021  \u001b[92m☑\u001b[0m 476663021 \n",
      "Q 802701553+979648591 T 1782350144 \u001b[92m☑\u001b[0m 1782350144\n",
      "Q 586448050+697612989 T 1284061039 \u001b[92m☑\u001b[0m 1284061039\n",
      "Q 108437553+97473372  T 205910925  \u001b[92m☑\u001b[0m 205910925 \n",
      "Q 273219443+460626864 T 733846307  \u001b[92m☑\u001b[0m 733846307 \n",
      "Q 638050220+671358568 T 1309408788 \u001b[92m☑\u001b[0m 1309408788\n",
      "Q 568652676+700857228 T 1269509904 \u001b[92m☑\u001b[0m 1269509904\n",
      "Q 820941149+702707555 T 1523648704 \u001b[92m☑\u001b[0m 1523648704\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 297s 7ms/step - loss: 0.0342 - acc: 0.9969 - val_loss: 0.0279 - val_acc: 0.9974\n",
      "Q 466266503+737237276 T 1203503779 \u001b[92m☑\u001b[0m 1203503779\n",
      "Q 631130030+13703950  T 644833980  \u001b[92m☑\u001b[0m 644833980 \n",
      "Q 429652799+694775779 T 1124428578 \u001b[92m☑\u001b[0m 1124428578\n",
      "Q 965755419+583735481 T 1549490900 \u001b[92m☑\u001b[0m 1549490900\n",
      "Q 523669124+820963802 T 1344632926 \u001b[92m☑\u001b[0m 1344632926\n",
      "Q 934198222+463218164 T 1397416386 \u001b[92m☑\u001b[0m 1397416386\n",
      "Q 539315795+406963916 T 946279711  \u001b[92m☑\u001b[0m 946279711 \n",
      "Q 763859012+327179911 T 1091038923 \u001b[92m☑\u001b[0m 1091038923\n",
      "Q 675253883+875050290 T 1550304173 \u001b[92m☑\u001b[0m 1550304173\n",
      "Q 487147613+903889640 T 1391037253 \u001b[92m☑\u001b[0m 1391037253\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 294s 7ms/step - loss: 0.0263 - acc: 0.9977 - val_loss: 0.0681 - val_acc: 0.9836\n",
      "Q 305562160+437714166 T 743276326  \u001b[92m☑\u001b[0m 743276326 \n",
      "Q 524866252+302794150 T 827660402  \u001b[92m☑\u001b[0m 827660402 \n",
      "Q 951641462+900746208 T 1852387670 \u001b[92m☑\u001b[0m 1852387670\n",
      "Q 638846704+948623934 T 1587470638 \u001b[92m☑\u001b[0m 1587470638\n",
      "Q 535398191+808741567 T 1344139758 \u001b[92m☑\u001b[0m 1344139758\n",
      "Q 66903276+543469261  T 610372537  \u001b[92m☑\u001b[0m 610372537 \n",
      "Q 714826086+622193888 T 1337019974 \u001b[91m☒\u001b[0m 1336019974\n",
      "Q 2293355+60769713    T 63063068   \u001b[92m☑\u001b[0m 63063068  \n",
      "Q 759457743+140564319 T 900022062  \u001b[92m☑\u001b[0m 900022062 \n",
      "Q 605146526+354249589 T 959396115  \u001b[92m☑\u001b[0m 959396115 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 245s 5ms/step - loss: 0.1420 - acc: 0.9660 - val_loss: 0.0292 - val_acc: 0.9964\n",
      "Q 996994735+889372448 T 1886367183 \u001b[92m☑\u001b[0m 1886367183\n",
      "Q 595194382+91663616  T 686857998  \u001b[92m☑\u001b[0m 686857998 \n",
      "Q 384611406+981548833 T 1366160239 \u001b[92m☑\u001b[0m 1366160239\n",
      "Q 331061244+934714976 T 1265776220 \u001b[92m☑\u001b[0m 1265776220\n",
      "Q 651549946+561049956 T 1212599902 \u001b[92m☑\u001b[0m 1212599902\n",
      "Q 75247802+611517590  T 686765392  \u001b[92m☑\u001b[0m 686765392 \n",
      "Q 873787875+440046523 T 1313834398 \u001b[92m☑\u001b[0m 1313834398\n",
      "Q 986405303+349787571 T 1336192874 \u001b[92m☑\u001b[0m 1336192874\n",
      "Q 884522577+325488361 T 1210010938 \u001b[92m☑\u001b[0m 1210010938\n",
      "Q 230000336+110163887 T 340164223  \u001b[92m☑\u001b[0m 340164223 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 296s 7ms/step - loss: 0.0390 - acc: 0.9942 - val_loss: 0.0240 - val_acc: 0.9984\n",
      "Q 163424056+795930139 T 959354195  \u001b[92m☑\u001b[0m 959354195 \n",
      "Q 273642516+349819089 T 623461605  \u001b[92m☑\u001b[0m 623461605 \n",
      "Q 283223866+685837911 T 969061777  \u001b[92m☑\u001b[0m 969061777 \n",
      "Q 723674135+647948012 T 1371622147 \u001b[92m☑\u001b[0m 1371622147\n",
      "Q 757265251+461239592 T 1218504843 \u001b[92m☑\u001b[0m 1218504843\n",
      "Q 665465620+536694995 T 1202160615 \u001b[92m☑\u001b[0m 1202160615\n",
      "Q 814574882+78172451  T 892747333  \u001b[92m☑\u001b[0m 892747333 \n",
      "Q 345679799+605235443 T 950915242  \u001b[92m☑\u001b[0m 950915242 \n",
      "Q 758363548+533480968 T 1291844516 \u001b[92m☑\u001b[0m 1291844516\n",
      "Q 771987220+890242957 T 1662230177 \u001b[92m☑\u001b[0m 1662230177\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 292s 6ms/step - loss: 0.0192 - acc: 0.9987 - val_loss: 0.0163 - val_acc: 0.9992\n",
      "Q 530332687+905933051 T 1436265738 \u001b[92m☑\u001b[0m 1436265738\n",
      "Q 871119583+633369980 T 1504489563 \u001b[92m☑\u001b[0m 1504489563\n",
      "Q 385094995+836452187 T 1221547182 \u001b[92m☑\u001b[0m 1221547182\n",
      "Q 273337622+856782576 T 1130120198 \u001b[92m☑\u001b[0m 1130120198\n",
      "Q 597385131+248024876 T 845410007  \u001b[92m☑\u001b[0m 845410007 \n",
      "Q 835966718+7401702   T 843368420  \u001b[92m☑\u001b[0m 843368420 \n",
      "Q 131221790+742055920 T 873277710  \u001b[92m☑\u001b[0m 873277710 \n",
      "Q 874973610+961302708 T 1836276318 \u001b[92m☑\u001b[0m 1836276318\n",
      "Q 345527321+36228732  T 381756053  \u001b[92m☑\u001b[0m 381756053 \n",
      "Q 946170445+836695115 T 1782865560 \u001b[92m☑\u001b[0m 1782865560\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 296s 7ms/step - loss: 0.0158 - acc: 0.9992 - val_loss: 0.0151 - val_acc: 0.9994\n",
      "Q 475447425+412787691 T 888235116  \u001b[92m☑\u001b[0m 888235116 \n",
      "Q 715554450+916004531 T 1631558981 \u001b[92m☑\u001b[0m 1631558981\n",
      "Q 193525081+498046422 T 691571503  \u001b[92m☑\u001b[0m 691571503 \n",
      "Q 425961603+270696640 T 696658243  \u001b[92m☑\u001b[0m 696658243 \n",
      "Q 590388482+167290847 T 757679329  \u001b[92m☑\u001b[0m 757679329 \n",
      "Q 659653489+769121813 T 1428775302 \u001b[92m☑\u001b[0m 1428775302\n",
      "Q 436924151+40097724  T 477021875  \u001b[92m☑\u001b[0m 477021875 \n",
      "Q 167019997+770415328 T 937435325  \u001b[92m☑\u001b[0m 937435325 \n",
      "Q 430608561+842808170 T 1273416731 \u001b[92m☑\u001b[0m 1273416731\n",
      "Q 401238523+577414094 T 978652617  \u001b[92m☑\u001b[0m 978652617 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 295s 7ms/step - loss: 0.0144 - acc: 0.9992 - val_loss: 0.0140 - val_acc: 0.9992\n",
      "Q 811527467+230445823 T 1041973290 \u001b[92m☑\u001b[0m 1041973290\n",
      "Q 514850352+528677011 T 1043527363 \u001b[92m☑\u001b[0m 1043527363\n",
      "Q 228726202+157092207 T 385818409  \u001b[92m☑\u001b[0m 385818409 \n",
      "Q 491442235+185141686 T 676583921  \u001b[92m☑\u001b[0m 676583921 \n",
      "Q 109080979+183692596 T 292773575  \u001b[92m☑\u001b[0m 292773575 \n",
      "Q 416325550+980877629 T 1397203179 \u001b[92m☑\u001b[0m 1397203179\n",
      "Q 738678950+952702266 T 1691381216 \u001b[92m☑\u001b[0m 1691381216\n",
      "Q 90786556+182209535  T 272996091  \u001b[92m☑\u001b[0m 272996091 \n",
      "Q 57549845+58095776   T 115645621  \u001b[92m☑\u001b[0m 115645621 \n",
      "Q 941899783+277541999 T 1219441782 \u001b[92m☑\u001b[0m 1219441782\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 292s 6ms/step - loss: 0.0132 - acc: 0.9992 - val_loss: 0.0163 - val_acc: 0.9984\n",
      "Q 733695352+166723913 T 900419265  \u001b[92m☑\u001b[0m 900419265 \n",
      "Q 494253397+826332373 T 1320585770 \u001b[92m☑\u001b[0m 1320585770\n",
      "Q 579436654+875145313 T 1454581967 \u001b[92m☑\u001b[0m 1454581967\n",
      "Q 526032664+126039717 T 652072381  \u001b[92m☑\u001b[0m 652072381 \n",
      "Q 165651474+385451799 T 551103273  \u001b[92m☑\u001b[0m 551103273 \n",
      "Q 429187014+845887604 T 1275074618 \u001b[92m☑\u001b[0m 1275074618\n",
      "Q 245289610+58932753  T 304222363  \u001b[92m☑\u001b[0m 304222363 \n",
      "Q 215989023+868294173 T 1084283196 \u001b[92m☑\u001b[0m 1084283196\n",
      "Q 923955468+262948857 T 1186904325 \u001b[92m☑\u001b[0m 1186904325\n",
      "Q 396843672+280818865 T 677662537  \u001b[92m☑\u001b[0m 677662537 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 292s 6ms/step - loss: 0.0366 - acc: 0.9908 - val_loss: 0.0119 - val_acc: 0.9991\n",
      "Q 499674589+262412006 T 762086595  \u001b[92m☑\u001b[0m 762086595 \n",
      "Q 749047986+449969907 T 1199017893 \u001b[92m☑\u001b[0m 1199017893\n",
      "Q 506102063+179195105 T 685297168  \u001b[92m☑\u001b[0m 685297168 \n",
      "Q 80161666+102982490  T 183144156  \u001b[92m☑\u001b[0m 183144156 \n",
      "Q 837164829+381297768 T 1218462597 \u001b[92m☑\u001b[0m 1218462597\n",
      "Q 180267503+150072134 T 330339637  \u001b[92m☑\u001b[0m 330339637 \n",
      "Q 706835331+952107178 T 1658942509 \u001b[92m☑\u001b[0m 1658942509\n",
      "Q 131073245+757207682 T 888280927  \u001b[92m☑\u001b[0m 888280927 \n",
      "Q 235779755+576616643 T 812396398  \u001b[92m☑\u001b[0m 812396398 \n",
      "Q 692660611+355761902 T 1048422513 \u001b[92m☑\u001b[0m 1048422513\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 296s 7ms/step - loss: 0.0106 - acc: 0.9993 - val_loss: 0.0092 - val_acc: 0.9997\n",
      "Q 414422958+87431125  T 501854083  \u001b[92m☑\u001b[0m 501854083 \n",
      "Q 93423049+810137162  T 903560211  \u001b[92m☑\u001b[0m 903560211 \n",
      "Q 781244279+230921812 T 1012166091 \u001b[92m☑\u001b[0m 1012166091\n",
      "Q 897267781+209165711 T 1106433492 \u001b[92m☑\u001b[0m 1106433492\n",
      "Q 80809588+300183866  T 380993454  \u001b[92m☑\u001b[0m 380993454 \n",
      "Q 194540150+764848123 T 959388273  \u001b[92m☑\u001b[0m 959388273 \n",
      "Q 831580496+625199947 T 1456780443 \u001b[92m☑\u001b[0m 1456780443\n",
      "Q 833646011+347597319 T 1181243330 \u001b[92m☑\u001b[0m 1181243330\n",
      "Q 266939665+169867556 T 436807221  \u001b[92m☑\u001b[0m 436807221 \n",
      "Q 444771481+988615492 T 1433386973 \u001b[92m☑\u001b[0m 1433386973\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 250s 6ms/step - loss: 0.0132 - acc: 0.9987 - val_loss: 0.0116 - val_acc: 0.9990\n",
      "Q 758362550+873884880 T 1632247430 \u001b[92m☑\u001b[0m 1632247430\n",
      "Q 989513042+327566044 T 1317079086 \u001b[92m☑\u001b[0m 1317079086\n",
      "Q 710672999+633536042 T 1344209041 \u001b[92m☑\u001b[0m 1344209041\n",
      "Q 211666156+392828313 T 604494469  \u001b[92m☑\u001b[0m 604494469 \n",
      "Q 931218876+148390595 T 1079609471 \u001b[92m☑\u001b[0m 1079609471\n",
      "Q 647053774+116221863 T 763275637  \u001b[92m☑\u001b[0m 763275637 \n",
      "Q 346515585+266074355 T 612589940  \u001b[92m☑\u001b[0m 612589940 \n",
      "Q 870548542+180526621 T 1051075163 \u001b[92m☑\u001b[0m 1051075163\n",
      "Q 856396858+912511175 T 1768908033 \u001b[92m☑\u001b[0m 1768908033\n",
      "Q 655874813+14929485  T 670804298  \u001b[92m☑\u001b[0m 670804298 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 294s 7ms/step - loss: 0.0091 - acc: 0.9994 - val_loss: 0.0098 - val_acc: 0.9994\n",
      "Q 752353872+937589676 T 1689943548 \u001b[92m☑\u001b[0m 1689943548\n",
      "Q 502159324+783851183 T 1286010507 \u001b[92m☑\u001b[0m 1286010507\n",
      "Q 467061838+337124295 T 804186133  \u001b[92m☑\u001b[0m 804186133 \n",
      "Q 50054050+491475791  T 541529841  \u001b[92m☑\u001b[0m 541529841 \n",
      "Q 798789313+241507210 T 1040296523 \u001b[92m☑\u001b[0m 1040296523\n",
      "Q 652890763+131310902 T 784201665  \u001b[92m☑\u001b[0m 784201665 \n",
      "Q 804082928+575947504 T 1380030432 \u001b[92m☑\u001b[0m 1380030432\n",
      "Q 976795763+694756631 T 1671552394 \u001b[92m☑\u001b[0m 1671552394\n",
      "Q 43710622+249222978  T 292933600  \u001b[92m☑\u001b[0m 292933600 \n",
      "Q 294229792+324342242 T 618572034  \u001b[92m☑\u001b[0m 618572034 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n"
     ]
    }
   ],
   "source": [
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for iteration in range(1, 200):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    \n",
    "    questions = []\n",
    "    seen = set()\n",
    "    expected = []\n",
    "    while len(questions) < TRAINING_SIZE:\n",
    "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                                for i in range(np.random.randint(9, DIGITS + 1))))\n",
    "        a, b = f(), f()\n",
    "        # Skip any addition questions we've already seen\n",
    "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "        key = tuple(sorted((a, b)))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        # Pad the data with spaces such that it is always MAXLEN.\n",
    "        q = '{}+{}'.format(a, b)\n",
    "        query = q + ' ' * (MAXLEN - len(q))\n",
    "        ans = str(a + b)\n",
    "        # Answers can be of maximum size DIGITS + 1.\n",
    "        ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "        if INVERT:\n",
    "            # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "            # space used for padding.)\n",
    "            query = query[::-1]\n",
    "        questions.append(query)\n",
    "        expected.append(ans)\n",
    "        \n",
    "    x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(questions):\n",
    "        x[i] = ctable.encode(sentence, MAXLEN)\n",
    "    for i, sentence in enumerate(expected):\n",
    "        y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "    # Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "    # digits.\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # Explicitly set apart 10% for validation data that we never train over.\n",
    "    split_at = len(x) - len(x) // 10\n",
    "    (x_train, x_val) = x[:split_at], x[split_at:]\n",
    "    (y_train, y_val) = y[:split_at], y[split_at:]\n",
    "    \n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    model.save('additionrnn_model.h5')\n",
    "    model.save_weights('additionrnn_weightsandbias.h5')\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        #print('Input' + rowx + ',Output:' + rowy + 'S')\n",
    "        #print(x_val[np.array([ind])])\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if INVERT else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
